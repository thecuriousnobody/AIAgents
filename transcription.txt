 Let's see. See. Let's see. For some reason your mic shows it's muted. Let's see. Okay. Okay. I can hear you now. Cool. Can I refer to you as Dr. Garimella? Yeah. Yeah. Okay. Thanks, good on for doing this. I'm so looking forward to doing this. Just give me one second here. Let me see if I could swap so that way I'm not looking so far away from you. I'm recording by the way, just letting you know. Let's see. Tiled. See. Spotlight. Spotlight. Spotlight. Spotlight. Spotlight. Spotlight. Spotlight. Spotlight. Spotlight. Okay. I guess if you don't care, I don't care. Is it okay if I'm looking sort of like that? Because you're. I was trying to swap. I was trying to swap the video out here and. First of all, thank you for doing this, kid. I was very close to my heart and I will. I will not put you through introducing yourself and think of that because I'll do all the post production and all add all this stuff and I'm. I'm actually planning this to be a little bit different than my usual podcast where I. Usually we'll just start with the conversation or give a little bit of preamble in this case. I think I want to kind of. You know, add some context and detail before we actually actually show the conversation and maybe. I want to add some parts of the conversation to kind of emphasize certain points. So, yeah, I won't put you through that. So if you don't mind, can I get right into it? Sure. Okay. First of all, how did you get into this field of research, kid? Like, so you know, WhatsApp University, if you don't mind me using that word. I guess I'll stick to WhatsApp University today or WhatsApp today, not Twitter so much. We can talk about Twitter too. What motivated you to get into this research? What brought you to this space? Yeah, yeah. So, you know, I have a background in computer science. I'm a PhD in computer science and you know, I was doing a lot of work on. Studying things like, you know, political polarization in the US. The usual stuff like with the Twitter data that everyone was drawing. And this was back in 2017, 2018 when you know, these like ideas of like misinformation. Here in the US, especially, we're just picking up like, you know, okay, misinformation research has been done for a long while. But, you know, especially after, you know, Trump and Brexit and all of these like in the US, like academic interest in misinformation as a topic, like really skyrocketed. There's a lot of this being a lot of interest in this space. And that was when like, you know, one of the summers I was in India. And I think it was the summer of 2017 when you know, there was suddenly a spade of incidents of like offline violence, like you know, rumors about childhood napping killing people like real violence and lynching things like that. Like and this was just in a very short duration around like 30 to 40 people got killed and many, many people were injured. So, this is in various parts of India all across India. And the root cause of this was just one rumor about one video about, you know, your children are being kidnapped by strangers coming to your village. Be careful. Right. So that was the rumor that is spreading and anyone who entered a village just was, you know, attacked and this really spread like wildfire. And that, you know, really got me thinking about like looking at WhatsApp. Okay, obviously there's very little work on WhatsApp. Primarily there's that bias because you know, it's not really that widely used here in the US. And like most academia really just looks at Twitter, which is much more easier to get data from and things like that. And so there were two up to real reasons. So one is just a selfish reason that you know, this is a new platform like new data, new interesting insights. So that was one. The other is just, you know, this sort of like, you know, online information leading to offline consequences. That's a really new idea. I can back there. I'm talking about seven years ago, even like just just to give you some context like even until recently, like you know, until the Jan Six, you know, thing happened. Like people would not really care about misinformation as as an offline consequence, right? Like so people would say, yeah, okay, what what if like people in a small amount of people consume misinformation? Like it's just an online phenomena, but online consumption leading to offline activity. I mean, this has happened in India and in many, many places in the global south quite a bit. But you know, in the US, like, you know, many big academics, like really you did not consider this as a big threat until, you know, this. This, this, this, Jan Six, 2020, what happened? So, yeah, but you know, these were the two of the reasons why I got into this around 2018 or so. Yeah, I mean, I, my, I also want to add to the conversation. I've seen very personal effect of this. Kid, on is that, you know, I've seen my parents relationship between themselves. For example, appearance relationship with their friend circle, my, my very close friends. And if they're ever going to watch this, they might, I might get some hate, but. But, but it's a real thing, right? Like where, you know, religious polarization, religious messaging. In like, like one of the things you said 82% one of the research, I went through all of your papers and some of the statistics were like mind blowing where even after being. They get re-shared and things like that and. And even a relative circle, you know, my, my cousins and my uncles and so forth. And it goes on to this very day, right? Like it's happening today. Information that's just manufactured to rouse your emotions, manufactured to get you to vote one versus the other way and so on. So this is very personal to me is, I guess, my point. And I guess I want to dive right into it. Like what can be done about this kid on. So you've laid out a bunch of research stuff, practically speaking. Can you kind of walk through, you know, let's say, you know, from a technical perspective, let's start with that. You know, let's say some entrepreneurs want to do a startup or a technology or plug in or some kind of software. What can I mean, of course, I'll put links to the papers and stuff like what can be done from a high from a 30,000 foot view. Yeah. Okay. So maybe I'll just give you some context of like why this is a very difficult problem, particularly for WhatsApp, right? Like our platform like WhatsApp and the reason is enter an encryption. So this is enter an encryption on WhatsApp because of which so what what that means is just simplify. If I send you a message, if I sent you a message on Facebook, you know, if I sent you a picture of a cat, Facebook will know, okay, you know, a center message to be and it's a picture of a cat, right? But on WhatsApp or other encrypted platforms into an encrypted platforms, you know, if I send you a picture, Facebook or WhatsApp just knows that I sent you a picture, but they don't know what is in it. It could just be a cat picture or it could be a hate video, right? Like so you don't know what is the content of the information that is spreading on the platform. So that is a huge problem in moderating any sort of content. So on Facebook, for instance, the Facebook platform, you have a lot of moderators who can, you know, or content is flagged and then the moderators look at that and say, okay, this is hate speech, this information. This is whatever false information and they can down-track it, take it down, label it like they have a bunch of strategies to work with that on WhatsApp. There's no such infrastructure. Firstly, okay, there's two sides of it. So one is, is this necessary? Like, yes, right? Like there's this enter and encryption provides privacy where nobody is able to look at any of your personal conversations. So that is, you know, a solid point on WhatsApp's behalf that, you know, they're saying, okay, you know, we don't want to look at your content. We don't want to do any sort of personal moderation. Okay, so that's one end of the spectrum. The other end of the spectrum is the cases of examples that I told you, right? So we knew that this particular video was causing riots in all these villages across India. Hundreds of people are being injured, but WhatsApp cannot do anything or does not want to do anything about it. Because, you know, these sorts of really opposing ends of the spectrum. Okay, so this is the problem. Now, what can be done about this? So, you know, obviously, you know, we all agree that, you know, encryption privacy is a good thing. We don't want to break encryption. You know, for instance, certain governments, like the Indian government, the UK government, Australia government, they have cases where they're saying, you know, you know, what's happening is being misused for child sexual exploitation. Right? Like so you have to break encryption in these cases. It's usually, you know, if you break encryption, you're giving like a pass, right? So encryption is a box. If once opened, you can give access to anything. You just, you cannot just say, I'll only give you access to, you know, CSAM content. You, it's a, it's a, it's a, you once you open it, you, you're breaking encryption forever. So that's the problem. So what can be done? So there's a multiple way. So one is what the platform can do in terms of the levers that the platform has in. Designing solutions. And what's up has done a decent enough job here. Like for instance, they introduced friction in reducing the spread of information. Right? Like so, you know, one thing you can do is, you know, if a piece of content has been forwarded many times, like, you know, forwarded a certain number of times will introduce friction to make sure that this is not, you know, massively forwarded over and over again. Right? And what they've done is they said, okay, you know, you can only forward to five people at a time. And if the content has been forwarded more than five hops from the original sender, then you cannot forward it more than one person at a time. Right? Like so that, you know, it does not really fully solve the problem, but it introduces friction. And we have some word that shows that this really slows down the spread of the information. Like by an order of magnitude, like so you know, if it would take, you know, 100 hours previously to spread some information, now take 1000 hours. So that's a really significant amount of change of time. Like how long it takes for information to spread. Right? And that's a very good level. But the problem is like, especially given how invested actors are like various actors like political parties, for instance, are in this game. Right? Like so, you know, especially, you know, there's this research on my research, but also some other word by the Washington Post, for instance, in India that shows that just one political party in India has like a hundred thousand people in one state working on social media activity. Most of it is just what's that? Right? So if you have a hundred thousand people just sitting and forwarding content or creating content and forwarding content, they can just keep doing it right. And so one at a time, there's nothing stopping it from stopping them from doing this and they can eventually, you know, spread their information. So it's not a complete solution, but it is one solution. Like so this sort of forwarding limits are a good solution to a majority of us. And like 99.99% of WhatsApp is not people from political parties. It's people like you and me who are using it normally. We don't have any malicious intent to spread this rumor, particularly like a lot of people just spread it out of concern, you know, like the same with the child kidnapping rumor and people just shared it out of concern that, you know, any child must be protected. Right? So, so you know, it does work to a large extent, but it is not a solution for certain types. Like so if there's propaganda or disinformation or hate speech kind of thing, which a certain political party or an invested actor wants to spread, that is not going to solve this problem. Okay. So that's the top down solution. There's very limited top down or what the platform can do. So that's one thing that WhatsApp has done very well. That's good. The other is in terms of more bottom up solutions, which is what the users can do. Right? Like so the what the users can themselves do. And one solution there that again WhatsApp has experimented quite a bit and invested and spent a lot of money on is just digital literacy. Right? Like so to help people know. And I think you know this podcast one example is spreading example like awareness and digital literacy. What's app has taken like you know full page ads and newspapers saying you know you have follow these rules don't don't you know if it's arouses your emotions like it's most likely to be false. You know those are simple tricks to make sure that people don't forward you know misinformation rumors of hate speech kind of content. But it's not I mean they have advertised it during the elections and they they're done right like so they don't want to do too much. Like I mean it's not it's not cheap but it's also not a long term solution. The other thing that they're doing is developed these things called tip lines. I don't know if you've heard of these. So these are just hotline numbers. These are like WhatsApp numbers that a lot of fact checking agencies in India have. And if you get any misinformation. Let's say I get it in Telugu. I can forward it to a Telugu tip line that's run by a specific fact checking organization. But one thing that the reality of these again there's a lot of money that's being spent on these to maintain these tip lines to build technology to make sure that these tip lines work and pay for the fact checking efforts that go into this. But the truth is just the scale at which these tip lines operate is very very very small like so again for a like WhatsApp is so big. So just in India over 50 billion messages are sent every day. Just just that's the scale of WhatsApp and you know I was working with some tip line like the that was run during the last election like 2019 election. And the scale was like around a hundred thousand messages that we got to that tip line in three months right. During the election period in three months you got a hundred thousand messages. Just looking at those messages. Okay, might look like a lot hundred thousand. But if you look at the overall scale. People are getting. Yeah, talking about you know trillions of messages over the three months period. And you you're fact checking or the amount of messages you got for fact checking or like you know, other tasks. It just does not scale well and most likely the people who consume misinformation people like my mom or my my family. They don't know of this right like they don't know of any questions of the tip line or they don't even know that okay. You know this could be misinformation. Let me ask this fact checking tip line or whatever right. So it just does not scale. And so okay, so I told you like these two examples like one is a top down friction. Okay, that works to certain extent. The other is bottom up like digital literacy kind of thing that also you know works to some extent it does not work fully. What else have we trying now like you know we are we are trying other solutions like variance of these digital literacy kind of like that help in. Scaling up these you know, awareness and understanding of people like so for instance you can. One thing we realize in our research is a lot of the misinformation like around 40% of it can be very easily debunked like so these are just old misinformation that has already been fact checked. That is just reshared and res spreading right like so this child getting rumour. It started 10 years ago even if today if you open WhatsApp it will be really spreading widely right like I just don't know that this is already been debunked. And so if you can provide tools that enable the connection of okay this video or this image has already been fact checked easily. It can be make that happen like so whatever tools you can provide to enable users to connect fact check content to misinformation information that people receive. I think that's an easy solution like that where you're not giving the burden to the user but you have some sort of technology that enables the sort of matching between fact check content and content that people receive. I think that's an easy solution that you know okay users can quickly say oh yeah okay this seems to have been fact checked here and this is a fact check. So you know that that could be a solution so we're trying out solutions like that but yeah skip these is not easy so easy interactors. Yeah so can you speak a little bit about that is it on your phone I know one of the papers you mentioned that in a kind of a localized fact checking database you have on your phone it can sort of you know be filter out messages or at least flag it for you. Is that what you're referring to? Yeah so that's one way of operationalizing it and implementing it you know so yeah the solution that we proposed there is you know it's a it's a it's a instead of having like you can have a small database on your phone of hashes of you know images videos and text that was already fact checked. You don't even have to have all the videos and images you can just compress them and represent them as hashes this is very very small it could be you know one megabyte for like you know hundreds of thousands of images and videos of a one that can fit on your phone and you can do very efficiently check like okay is this you know if I get an image like is this image in this database or not this can be done on the device you've shown you know that this works very efficiently. And you can detect a lot of misinformation that's where the 82% number comes from so you could prevent a lot of the spread of the information just by making sure you do this. So one thing we realized after we wrote that paper is okay this is technically possible but this is practically a very like contention like there's a lot of debate and contention especially from the privacy advocates on this. So so the issue there like so the discussion came out after Apple introduced or was trying to introduce some of these kinds of same kind of tools which like they work on device but this can your gallery for child sex abuse material and then if there's some match then they reported to the authorities kind of thing and there was a lot of discussion Apple actually backtrack like not doing it and the discussion there was again this sort of privacy versus criminality right okay maybe they're not going to be a lot of people. Maybe there's 0.001% of all the population is has this sort of content like you know child sex abuse content but now you're scanning everyone's phone and matching it with your database so how do we guarantee that you know you don't inject some sort of like content that you know maybe in India or in some other country that's an authoritarian country they'll ask you to inject some sort of benign video like I don't know Trump getting shot or like Biden's. They don't want anyone to see and they will be able to then track okay you know all these people have this on their phone right like so how do you make sure that this database that you're creating centrally that you're then shipping to the phones is full proof like so that is very hard problem like and how do you ensure that it never it will never ever have anything other than you know these sorts of child sex abuse material or whatever right like so you could be easily misused. So there's there's that sort of thing so it's technically feasible to do this but it's not practical and you know also also you know if for instance windows or Microsoft is scanning your computer all the time for what upon you have or what other stuff you have that's not that's not a world we want to live in right so that's the kind of argument that was made by all these privacy advocates and I really believe that yeah that may not be the best solution for that that we should think of as a solution. Yeah so I guess what I'm hearing is that this is an ongoing challenge and we just have to from a technical perspective we just have to keep trying our best and designing better algorithms and better ways to educate people and it's really no airtight solution at the end of the day. Yeah yeah yeah I mean this is an ongoing area of research right now we're working on some other variants of that like where you know if you can again like this is a very practical like where like I said like a lot of misinformation is very easy to debunk but many people may not have the tools to do it but you know if you think of you know your local network like so maybe my mom does not have the capability to debunk all the misinformation that she gets but you know I'm in her network and I have the tools to just reverse image search or Google. Google search specific set of keywords right and she will trust me more than she will trust some random fact checker right similarly take any network is very good chance that you know someone in the network is capable of doing these some some some some some some some very local basic tasks that enable like fact checking right so we are we're now trying to deploy some of those in a way like if you can recruit people in a network and try to get some sort of information on what they're receiving and can you correct this sort of misinformation. Locally without having to send it to some sort of central factor or overwhelming the resources and all of that kind of thing but yeah this is an open problem there's no clear solution that works and scales right now but yeah there's a lot of work going in this case yeah. Yeah so I think the follow up to that is the second part of it I was my question was okay so on one hand you've got technical solutions you've got research R&D folks like yourself are working on that but from a social cultural perspective so you know a lot of times I look at the messages being forwarded to me it's just confirmation bias right like this there was already a pre existing kind of belief or notion about a certain group of people you received content it looks legitimate to you and it's like see I told you this is actually going on so to me it's like the old the you know sort of the this also amplifies this right like you can't that's a harder problem to fix how do you fix you know where people are more diligent about looking at their data are diligent about this stuff because I mean what's happened digital proliferation is a relatively new thing in India right like and then we've got a mass illiteracy that's that problem compounding on all of this so it's like the have you ever thought about these kind of social cultural issues and how we tackle that yeah yeah so so yeah how are these things that I mentioned to you are like the solutions are only for people who you know are willing to listen and get it right like and again like we have data that proves to what extent like these are feasible and you know again like if you talk about misinformation particularly a lot of it like it let's say you know 80% of it is benign misinformation just you know okay drink drink hot water and you'll cure your throat or whatever or you know whatever this child is being kidnapped sure killer maybe maybe I'm part of maybe I'm part of what's our groups where there's just blatant hate stuff right yeah yeah you know and it's it's pretty incessant like I open up my watch up in the morning when I wake up there's the first sort of thing I do catch up with all the stuff that has happened in India and it's it's a queue of literally 40-50 messages of just like that like 80 I mean like you said 80% of it is just you know just read me thrown at you right like and it's like these are educated people these are people to who went to college got a degree or in business or whatever I T and all of these things it's it's not a in my opinion at least in my personal experience anecdotally speaking. I mean I have seen this manifesting in awful ways people breaking off ties relationships getting afraid and things like that. Yeah yeah yeah I mean personal experiences why my differ like my experience is similar like you know most of our misinformation I get is very clean like you know politically oriented politically motivated and those kinds of behaviors and beliefs are extremely hard to change right like so if you tell someone who's a ex party supporter to say you know you know your party whatever ideology or is wrong like that is not going to work is a very very hard thing to change and there are certain techniques that you know may change that long term and things like that but whatever solutions we are thinking of are only for this sort of more softer kind of misinformation which is not based on like you know where people have pre-made opinions and and again like you know at the scale of WhatsApp even solving that like maybe it's not 80% maybe 60% maybe it's 20% right like so it doesn't matter like even if you solve that 20% like you're you're really solving at WhatsApp for resolving at WhatsApp scale a large problem that's that's where how I think about it like definitely you know there's there's this again like a significant chunk of it where people have pre made their opinions and again like you said so much more system makes much more societal kind of issue where where again maybe the solutions do exist like where you know for some of these problems like the issue is just more for top down pushed like you know one individual person person like if they can come out and say you know this is not what we stand for or this is not see you can change mass opinion just based on one leader one guy one person one entity saying something right so those kinds of things or even sorry entropy but even like you know prioritizing critical thinking education in in primary school could be a pretty good solution yeah yeah I mean definitely I think that's that's something that has to be like yeah you know but that just ignores a lot of this sort of last generation like yeah your friends my friends my family your family kind of thing where there's still like you know these beliefs are so rampant and we see them quite often like those are yeah definitely hard to change yeah um the earlier I mentioned the political parties you know having these IT cells and things like that and I guess I mean these parties are very open about it they're very brazen about it right like they they have a IT cell you know headhound show who speaks in public and things like that various political parties have this stuff like is that I mean what is your opinion about that whole thing where you know these these IT cells are generating content to you know throw red meat at you to rouse up your emotions and so on um is it can if if legally we introduce something like well this is illegal is that too much of an overreach legal overreach or what is your opinion on that sort of thing yeah yeah so I mean one thing is there's no clear causal link between you know your social media presents and outcomes right and our political parties in India know that there is there is some non significant amount of uh uh uh effect for sure like but we saw in the recent election right like so I mean we have been doing this very very large scale WhatsApp data collection that we're monitoring thousands of WhatsApp groups most of them are just you know pro BJP content but you know the results don't really reflect that like and similarly you have hired like multiple instances of this like Karnataka references where they they really went in all in like with a lot of campaigns and that and so on again the result did not come to their benefit like so when you're thinking about like banning or any regulation or any of these kinds of things um yeah you don't really like we don't clearly have evidence that shows that yeah having this very popular IT cell that producer all of this content really makes uh makes a difference really okay so I I was thinking there's a fairly easy I mean earlier you mentioned that example where this one video circulated and there was riots and there was the the outcomes are pretty negative I mean you could probably draw some kind of conclusions from that right similar conclusions it's very hard to say like this caused this like because you know electorally like you know there's all these rallies happening and much so what people consume is not just social media especially in a country like India social media still you know okay the people are on social media but not just on social media right you still have like TV you still have offline sources you have a bunch of other sources it's very hard and again like you know you can see the results right like so in UP like where we have data from like most of the viral information like most of the content that is forwarded many times from WhatsApp significantly tilted towards pro BJP but you know that also like more like 50 50 right like so it doesn't necessarily mean and and and I talked to you know some of these parties like people SP or Congress and their strategy was that you know social media is too expensive, too hard to maintain especially at scale like so you can have an IT cell but if you have to constantly keep creating content and constantly create content that engages people that's a very big expense okay they would rather spend that on offline campaigning door to door reaching out to people you know spend giving money to people whatever like your other modes that pay more bang for the park right and it's not completely clear that you know this pace off maybe it does in the long term right like and that's what we were talking about maybe over the 15 years since social media has existed like the strategies and the kinds of content and narratives that this one party have created they don't you know if I have some ups and downs here and there but that sort of narrative might really you know is this now a reality in India and you know no longer like that 15 years ago you could not have said some things but now you can say those things much more openly because you know maybe social media conversations have normalized some of these that you know it's people feel okay to say some of these right like so those are the long term trends might exist but short term electoral gains are not like may not I don't think it's very clear that you know it really has an effect yeah I mean to that point like the overt and window could be shifted constantly right like by if you're constantly bombarding kind of one kind of information to your constituents you could clearly shifted I mean it's not a direct cause and effect sort of thing but over the period of time like you said and that's what I'm observing anyway because I you know pre 2015 I think is worth mentioning that most of my friends I would speak to were like kind of a moderate spectrum of beliefs you know it's like okay Hindus Muslims Christians whatever all of this stuff was sort of you know accepted as part of Indian life this was just you know we're a multicultural society blah blah but then as you've seen in 10 as I've seen in 10 years since 2015 it's pretty clear that the overt and window are shifted like it's it's even though the facts don't you know reflect that on the ground the attitudes have changed you know it's like the moment something happens you just lunch to this one conclusion that this must this group must have you know done this or that anyway I think we agree on that I guess I here's a personal question for you kid and you you've studied in India you did your I guess your PhD in Finland in Europe and then you did a postdoc in MIT and things like that so all your you kind of spend a lot of time in all of these three continents what have you seen from a societal perspective from a cultural perspective that makes certain groups of people susceptible to misinformation yeah yeah that's very interesting like so I mean there is there is research that like talks about like the types of people who are susceptible to misinformation or to conspiracy theories and narratives and things like that so I mean it depends on the on the context and the types of narratives and things like that like but I think overall like especially you know there's not really a lot of work in the Indian context even though you know it's such a huge and important population like it's not not much I think some of my work in this space some some other political centers are doing some work in this space but not not a lot in the US like even recently there's some very high profile like you know large scale studies that show that you know most misinformation consumers are older people who are on the right and I offer you know have a certain type of belief system and this is in the US or in general in the US this is all okay but my assumption is my again that's where I was coming to like so all these groupings have a certain common phenomena one is okay lack of digital literacy like like so the older generations may not have had as much exposure like it's a new shock to them coming on to this medium as as you know someone who's a bit more younger who's a much more digital native who's seen some of these like you have suddenly again this is one of the reasons why it's a big problem in India is because you know of that sudden introduction of social media a lot of that digital literacy narratives that you know we you and me have come through 15 years ago with the email scams and now a lot of people are going through that phase you know again it might settle down down the line you know maybe five 10 years down the line that people are in Accolated to some of these very obvious types of misinformation but again you will try to threat my damage emerge so that's one the older folks the other was more republican and this is where like most of the impact for misinformation is our beliefs and misinformation are far because of top down narratives right like so whatever you know your party or your leaders you know that sort of thing that the tone is set there and the beliefs propagate down like so and again in the US like one of the biggest pervers of conspiracies misinformation narratives any of is mostly on the right I mean obviously they exist on the left but it's a really yeah significantly more on the on the right and this has been not just one paper across multiple platforms across multiple studies this is consistently found that you know it's the same on the right it's much more it's the same in India again like you know there's only one party that is that is a source of a lot of the misinformation narratives and so on so it's it's I we don't have clear evidence in India but I assume like it's going to be similar or trends like you know certain types of demographics are much more susceptible and there's reasons for that yeah I wonder what what it is about being right leaning that makes it susceptible to misinformation okay so now we're in the co-haul for a of you know AI getting in solving a bunch of problems I use it a lot for my work I see a lot of potential to solve a lot of problems where human intervention in the past was required now you have the sort of artificial synthetic intelligence that could discern certain things for us I guess what is what work have you done or are you doing work on that space and where do you see long-term that helping us out in this misinformation landscape yeah yeah so I mean definitely I think you know AI as a solution is definitely that that's something that you know we are exploring we're not there yet especially for you know a context like like in India a context like WhatsApp which are which have you know the following charges one is it's a conversation right like so it's you need to understand the context and the context is very very dependent on like you know the context of the group like so if you and your friends are in a group you could call someone something insulting but if it's the culture of the group and it's an internal joke then it's not hate speech versus if I am in a public forum and I call someone something so you know you the context is very very it's very context dependent that depends on who else is in that group and your offline connections to them and there's no AI that can really ingest that or get that information right so that's that's one like especially in a chat context it's very hard to detect and understand context the other problem with you know platform like WhatsApp is again we have data for this over 50% of the content is non-textual again if you see your WhatsApp you will have the same kind of belief like a lot of it is not just text it's you know videos images audio emojis you know that sort of thing which is not the same as your Twitter so your Facebook you know that's sort of thing they're mostly sexual conversation again the tech and AI for text is much much more advanced than what it is for these sorts of multi-model setups right like especially if it's images that go along with the text or video that goes along with the text and both of these combined actually form them its information rather than one of so that kind of thing again we don't have it right now might exist in like a few years down the line but the third problem here so the first is context the second is multi-model the third is multi-lingual so not context again it's Hindi or other low resource languages where you know it does not work as well as it works in English again people are working on solutions for this might be better you know three five years down the line but not today yeah I mean to the point of context I use a lot of agentic infrastructure I use career AI you know I spin up agents for all sorts of things and when I use it even on a one-on-one chat or even within the agentic infrastructure it seems to hold context really well I guess my two senses that the intelligence is at the level of group of research assistants I sometimes I'll say you know I'll spin up a bunch of agents and say I want you to you know come up with a script for a podcast I'm I'm you know this these are the parameters this is the person blah blah blah and I mean I use the cloud 3.5 sonnet which is really good right I'm sure you've heard of it or use it and these agents I use I I mean they are on par or exceed human kind of cognizance the ability to understand nuance the ability to sort of contextualize certain pieces of data and I guess my two senses that I I see I'm a huge I'm a huge AI a fan because it helps me spin up these research assistants for a variety of things I do you know artwork blogging and podcasting all of these things it helps me in every little context and it also understands that sort of contextual knowledge so anyway you you you won this award which is let me let me read it out here the uh dynamic glance distinguished young research award can you speak to that yeah yeah I mean it's an award that's given every year at this uh conference on web and social media it's one of the premier conferences on uh social computing or your computing for social good kind of purposes and yeah it's a it's a very prestigious award congratulations man yeah fantastic yeah yeah sorry sorry I didn't mean to cut you off I mean yeah so it's it's it's a it's a far work done in this space on like yeah how do you use you know computing technology for social good purposes like you know trying to you know yeah use the AI for instance of solving some of these societal problems like misinformation I think my work specifically in the global South arena like you know countries like India and Brazil I think that's that's what actually helped me get that oh wow and I guess looking forward um I guess what would be your advice like you know people want to get into this space I mean I'm looking at WhatsApp echo chambers or you know that you reference by the way can you can you can you describe in very kind of plain English terms what is it what is an echo chamber so yeah an echo chamber is a phenomena where okay if you think of a physical echo chamber that's where the term comes from an echo chamber a physical echo chamber is place where you hear an echo right like so you speak something and you hear you're in a chamber where you just hear your own voice over and over so that's an echo chamber in the context of social media it's a place it's a metaphorical place where you're surrounded by people who produce ideas that are just agreeing and similar to yours right and this more and more often keeps happening on social media not just on WhatsApp but you know on other platforms and the reason why bring in WhatsApp and other platforms is a part in so on other platforms like Facebook or Twitter and so on uh it's the algorithms that are doing this right like so there's more and more you know you choose to interact and engage with certain types of content and the algorithm really say oh yeah Kiran likes this type of content let me show more of this so we get more engagement and that creates these sorts of echo chambers where you may not necessarily choose to engage a certain type of content but if you open my YouTube feed now it's all a certain type of content and that's an echo chamber that these algorithms are trying are creating and there's there's a lot of like uh uh the awareness within the social media platforms about this and trying they're trying to you know diversify your feeds and all of that uh there's a lot of effort in there but uh you know if you take WhatsApp there's no algorithms right it's all personal choices and there's a lot of like uh uh echo chamber behavior where okay the groups you join the people you interact with they're very limited and and this has to do with like again your source of cognitive dissonance and the the the term that you mentioned earlier so I if I have the choice I don't want to go and deliberately spend the limited time that I have like uh spending you know things on things that are dissonant to me and I feel uncomfortable and it spends my brain cycles or whatever I would just want to enjoy my you know things that I agree with and I don't have to put a lot of mental effort in so that's that's the reason why these things happen. Okay so if I have to interpret what you just said it's we becomes susceptible to echo chambers because we don't want to use a lot of cognitive resources it's much easier to consume something that gives us easy answers something that is just kind of fast food for the brain so to speak and I guess what is your I guess your recommendation or your um how do we inoculate ourselves to be a little more critical thinking about this how do we at a personal level how do we take responsibility from a personal perspective. Yeah I mean before we go there like one thing to set the context is okay obviously this sort of echo chamber behavior is not dangerous in all context right. I click if I'm watching cricket all the time and it's only cricket that's being recommended that's good like I mean it's not like you know somehow I expect Facebook or YouTube to show me more of and or goal for something I'm not interested in that I don't want to do so this echo chamber behavior is only dangerous when it comes to large scale political content right like where you know society itself is never in a position to deliberate and discuss with others and they're not consuming information from the other side right like and so it's only for political content specific types of political content that this type of deliberation is important and necessary and and so that's the area of concern but overall if you think of how much politics people consume it's not that much like so you know you again like the studies that show maybe you know 5 to 10 percent of the information that people consume on average again you and me might be outliers so again like so certain people might be outliers but most on average like most people don't consume all their time consuming politics or news like it's it's a small amount like 5 to 10 not negligent like a small amount but not negligible right like so okay so given that like okay how do we inoculate ourselves like one thing is again like you have to give it to the platforms like I was just talking to someone from one of these platforms and to a certain extent like they really over corrected in a sense like to the fact that you know this is happening on for political stuff I mean there's been research that was done like there's this other term called the filter bubble which is about like the same thing you know algorithms filtering out content that create a bubble for you where you're only seeing content that agrees and a lot of these platforms the big ones specifically you know Facebook Google they're really aware of this problem and they do have these factors that ensure that you know people get diverse viewpoints at least from the algorithmic perspective but again like there's this two parts to this like one is the user choices like what we choose to consume and what we are interacting with and the other is what the algorithm is recommending to us the platforms can can what with the algorithmic part of it making sure it's diverse but you know it's also up to the users to choose you know what what they want to consume and obviously you know it's it's not an easy thing you know it's not a one-shot that fits all like you just being aware of the fact that you're in an echo chamber like off you're choosing I think is I get that's a literacy point there like a lot of people may not just even be aware of the fact that you are in this chamber where you're not getting other viewpoints right again it happens to all of us very like so you know whenever we see surprises in like I don't know I thought Trump was doing really bad during the last cycle but he got like 75 million votes I again in my echo chamber nobody told me that you know okay Trump had like 75 million people who were supporting him and that's that's a certain type of echo chamber maybe that's that's not a great example but anyway so you know just being aware of the fact that of and cognizant of where you get your information from and trying to be cognizant of that I think is is already quite quite good yeah it's also requires you to spend some energy like you were mentioning earlier it's the owner is back on you to you know be more responsible citizen but you know that I also like thinking about this whole conceptual echo chamber right like it's a it's a relatively I mean I learned it only I don't know 10 years ago or something like that which goes to show it's not a widely circulated term in the vernacular right we normalize echo chambers it's it's it's it's it's all it's very normalized at least in my parent's circle who are in their 70s 60s and 70s I mean if for them it's like what do you mean echo chamber this is this is the gospel I'm going to listen to this new source or whatever it is that's going to you know explain to me what I already know and they're going to confirm my biases and so on and yeah go ahead sorry sorry yeah sorry like I think on that point like you know echo chambers are they the existence right it's not like you know this is a new phenomena that social media is created you know like you were saying we're trying to have two channels to watch and I of course I watch the channel that that is why viewpoints and I also like offline echo chambers obviously my friends are going to be people who have similar belief systems means have similar political views as me so so so that's sort of thing already existed like I think this sort of mass scale you know platform induced echo chambers are those are the like difficult things and I think it's even a better thing that you know we're at a stage where on social media or in the digital world we have much more choices now so yeah just being aware of and you know having the exercise having to exercise that you know saying okay you know I've consumed this much from this particular source let me just try to diversify just that knowledge I think is enough like I think that's that will really help in mitigating a lot of these issues yeah and I guess I get I'll ask you a personal question so illiteracy is a big problem India last time I checked the statistics it was something like close to 70% was the literacy rate in India something like that so nearly a third of India is you know illiterate unable to read and things like that they're watching they're probably consuming videos or pictures or what what have you I mean that is a mountain mountain of a problem by itself right and you're talking about digital literacy and stuff like that like that's a far milestone what are the I mean I guess how does this kind of rank in terms of like how do we do we prioritize just basic literacy to begin with and the other problem is in India we've got this kind of multilingual landscape right like and so you know you could you could be bombarded in various languages and how do you you know how do you translate these things in various languages it's a very compounding kind of a problem in India I mean I'm sure you thought about this yeah yeah yeah so so I mean again like when you think you got this problem over a long time so there's the last generation right like so that okay that we cannot really you know fix or educate at scale so a lot of the digital literacy or literacy initiatives like if you again stratify literacy by age group maybe younger education is much doing much better I mean there's so much money being spent on all these you know national education mission and so on so I think on the younger side it's close to 90% or so the literacy rate like below 18 so I and but obviously you know you're spending a lot of money on that but they're not really learning a lot so the quality of education that people get is really really low so obviously yeah focusing on literacy and improving the schooling system especially government schooling the public infrastructure schooling that's that's a huge huge task for India that needs to be fixed like but if you can incorporate these sorts of issues off like okay digital literacy I think you know I've done a spent a significant amount of my life in Finland Finland does it so well like they are one of the world leaders in doing this sort of literacy and they have now like they quickly embrace technology and you know now they're really trying to teach kids how to type instead of writing in cursive you know things like that they I think they really move very fast and do things in a certain way I mean probably it'll take a decade or so for a country like India to catch up but you know yeah we need to have policy that ensures that these things are taught from like your childhood like you know from your primary school so that again it's you cannot really separate now literacy and digital literacy because once these kids are at a certain level in school they're already gonna start using technology and so that's a become a part of life now so you cannot really escape that so you know it should be definitely tutorial and curriculum so schools yeah okay I guess my last question to you then or maybe my last question I know your time is is premium how much do you work with like these big big name companies in Silicon Valley and other companies that you know create these tools like you you're directly interacting with these people I guess what is your experience with that with working with that how open are they because they also have to you know balance business interest versus you know mass adoption of these of these platforms and so on like can you speak to that a little bit yeah yeah so again like you know my views on this are slightly I'm very very sympathetic to the platforms and the way they need to make decisions really yeah I'm very very pro platforms you don't think they're the villains no absolutely I really have a belief that you know this belief that they are somehow the villains who lead people to echo chambers or leading to teens getting you know whatever like that's a very misconceived belief like I mean especially running businesses at this scale requires a lot of decisions right like where you cannot optimize for one set of people and there's always going to be these sorts of edge cases where those kinds of things happen but overall I think they're very responsible and try to be responsible again I this belief is not like very you know I widespread like but my personal belief is that there are a lot of people you know in a lot of these companies that do genuinely care and it's not it's not binary like you cannot say okay they're maximizing profit so that's why people are seeing this it for it does not work that way obviously I mean there's so much so many dimensions here they have to somehow work with like you know regulatory pressures they would work with bureaucratic pressures they have to work with I know authoritarian garnish pressure is so in different parameters that they have to balance and it comes to you know when you know obviously can they do better yeah they can do better like are they trying their best like maybe they're not trying their best but are they really trying to really make things worse definitely not like and my personal interaction with a lot of people in these companies really gives me the hope that it's not just that binary thing where they're engaging they're they're they're profit like maximizing profits so they're they're ignoring a lot of this I think it's much more in the middle like okay so they have the levers over the algorithm right and like so what I'm hearing you're saying is that they have levers of the algorithm they are good actors they're doing their best to kind of balance all of these things but you know what you see in like other podcasts or conspiracy theorists or you know people who have an axe to grind with these social media companies is that oh they're they're tuned the algorithm up to just throw red meat at you is the is the notion right that that is absolutely false like again it is false it's absolutely false like it's probably false that you know you I mean again like this is a it's a regulatory problem for them right like they really are knowingly doing this or tuning to somehow cause harm they're going to jail right and they they really are careful about so you don't think they're finding loopholes which are just you know ethically wrong but legally acceptable again like I mean there might be specific cases where specific teams have had features may be tested in a way where you know maybe they like chose a specific thing that optimized for profits rather than social very specific cases but generally speaking like this notion that social media guides have a dial in the algorithm and they're tuning it it doesn't work that way I mean there's no one algorithm there's no one tune that you can do and again like I said any balance you do like to to to reduce certain type of harm will harm other types of people right like so there's no one steady state where this is beneficial for all of them and it's also not just the algorithm there's a lot of human choices that are involved in this right right a lot of people themselves are bad actors are deliberately or not like so you're choosing to consume certain type of content or certain type of you know engage in certain type of activity or post certain types of so there's there's a lot of this that they have to balance and it's very very very difficult if not impossible to find this fine balance where everything is all you know sunny and nice right like so that's impossible so yeah I think that's a very wrong notion that you know somehow there's evil guys who are trying to maximize profit if might happen I'm not saying it's not happening but it's not like I don't know it happens in one percent of the cases it's not the majority okay okay well that opens my eyes up a little bit I mean I had this very cynical view that you know it's a profit maximizing machine these Googles and Facebooks of the world you know they don't they don't care they as long as you know they can straddle the line of being legal legally acceptable they're going to take that line and ethical ethics could go out the window what I'm hearing you're saying is that they absolutely try to you know be on the ethical side of these questions or these they can amoreally you know gray area you know kind of areas where they they don't they're not bad actors they're trying their best but I think I think then the the takeaway I see personally is that it's a multi-pronged effort like we have to societally also engage ourselves to educate ourselves to read ourselves of misinformation biases and so on we have to attack this from like you know critical thinking perspective like how do we teach our kids to not fall for certain kind of narratives and so on at the same time we tune up all the levers on the technical side that's that's the only we're going to tackle this is there's no one-size-fits-all solution there's no magic bullet here yeah yeah it's as much self-correction as it is you know you're evil platform that is responsible for a lot of these right like so you know there's there's equal responsibility in in this and definitely again like you know maybe there was a time when you know everything was more fast and break things and they realized that's not the way that is I mean that's going to get them to jail so they don't want to do it anymore maybe it was that way back back in the day but it's changing yeah really things are not like that anymore like so it's much much more thought through especially at these big platforms and the big platforms are the one that really have have the most impact if they're extremely careful yeah yeah I speak into which really quick can you speak about that rwc score I found that very interesting how you could recommend certain kinds of content to people with opposing views and stuff can you talk to that yeah yeah so basically the idea there yeah it's exactly like a solution to the question that you had about how do people get out of their echo chambers right like and one of the findings that we had was obviously you know if you are you know just polarization is on a spectrum like so if you are an extreme right person you wouldn't want to be recommended something on the extreme left and you know you want you might want to be on the on the center right if you are on the center maybe it's okay for you to see something more on the extreme on the other side just to broaden your your perspectives and so on so we kind of operationalize this mathematically as a you know users as a network connections between users and then the flow of information as a random walk on that network basically it's a it's random walks are really really powerful it they exist all over you know networks in nature like for instance the Google page rank is basically just a random walk on a network that that gives you this sort of like if you do a random walk on a network if you end up in certain pages those pages are going to be ranked higher so that's kind of the the the property of this random walk so basically we do something like that and then we recommend people based on this sort of random walk and that has certain properties and saying that these are usually you know well-known trusted popular people from the other side with a certain type of opinions so you know if you recommend those sorts of people they're more likely to be engaging to the other side and more likely for you to be open to suggestions from the other side so that's what we find with this time okay well Kiran you're doing a great service to humanity as I see it keep doing what you're doing man I appreciate your time and your knowledge and let me know however I could be of future use to you I will absolutely try my best to amplify your voice however I can I won't spread misinformation I'll be as diligent as possible to help digital literacy I guess so thank you very much cheers yeah thanks so much yeah yeah talk to you later bye bye